# -*- coding: utf-8 -*-
"""mnist_fashion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RlAzTNGWpELnrE-NlneyHkFHTPp-8RfR
"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.transforms import ToTensor
from torch import optim
from torchvision import datasets

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize((0.5), (0.5))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5), (0.5))
])

train_data = datasets.FashionMNIST(
    root = "data",
    train = True,
    download = True,
    transform = train_transform
)

test_data = datasets.FashionMNIST(
    root = "data",
    train = False,
    download = True,
    transform = test_transform
)

batch_size = 128

train_dataloader = DataLoader(train_data, batch_size = batch_size)
test_dataloader = DataLoader(test_data, batch_size = batch_size)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

class CNN(nn.Module):
  def __init__(self):
    super(CNN, self).__init__()
    self.model = nn.Sequential( # Fixed: Removed extra indentation
        nn.Conv2d(1, 32, 3),  # 28x28 -> 26x26
        nn.ReLU(),
        nn.MaxPool2d(2),  # 26x26 -> 13x13

        nn.Conv2d(32, 64, 3),  # 13x13 -> 11x11
        nn.ReLU(),
        nn.MaxPool2d(2),  # 11x11 -> 5x5

        nn.Flatten(),
        nn.Linear(5 * 5 * 64, 128),  # Fully connected layer
        nn.ReLU(),
        nn.Linear(128, 10)  # 10 clase
    )

  def forward(self, x):
    return self.model(x)

# Instanțiere model, funcție de pierdere și optimizer
model = CNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Antrenarea modelului
num_epochs = 50
for epoch in range(num_epochs):
    # Antrenare
    model.train()
    running_loss = 0.0
    for images, labels in train_dataloader:
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Evaluare pe setul de test
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)  # Alege clasa cu probabilitatea maximă
            total += labels.size(0)  # Adună numărul total de exemple
            correct += (predicted == labels).sum().item()  # Adună corectitudinea

    # Calcul și afișare
    accuracy = 100 * correct / total
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}, Accuracy: {accuracy:.2f}%")